{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nFQXEcNOfOQr"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "def is_running_in_colab():\n",
        "  return 'google.colab' in sys.modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wx5jiPYKZKZD",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%pip install --upgrade pip\n",
        "%pip install wandb\n",
        "%pip install randomname\n",
        "%pip install --upgrade torch\n",
        "%pip install flash-attn --no-build-isolation\n",
        "%pip install --upgrade datasets\n",
        "%pip install --upgrade accelerate\n",
        "%pip install --upgrade peft\n",
        "%pip install --upgrade transformers\n",
        "%pip install --upgrade huggingface_hub[hf_transfer]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ugVtg-pXRC9"
      },
      "outputs": [],
      "source": [
        "import randomname\n",
        "\n",
        "name = randomname.get_name()\n",
        "notes = \"\"\n",
        "\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CHCIvTfnc94R"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import login\n",
        "import os\n",
        "\n",
        "if is_running_in_colab():\n",
        "    from google.colab import userdata\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "else:\n",
        "    hf_token = os.environ['HF_TOKEN']\n",
        "login(token=hf_token)\n",
        "hf_name = \"username/FrozenLake-\" + name # rename as needed to point to your repository on huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "onMMCEcclKKN"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "import os\n",
        "\n",
        "if is_running_in_colab():\n",
        "    from google.colab import userdata\n",
        "    wandb_token = userdata.get('WANDB_TOKEN')\n",
        "else:\n",
        "    wandb_token = os.environ['WANDB_TOKEN']\n",
        "wandb.login(key=wandb_token)\n",
        "wandb.init(project=\"FrozenLakeTrainRL\", name=name, notes=notes, save_code=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "aU7d7gOPdMli",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from peft import TaskType, get_peft_model, PeftModel, LoraModel, LoraConfig, IA3Config\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "from huggingface_hub import HfApi\n",
        "import os\n",
        "\n",
        "base_model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "adapter_name = None #only specify if you want to pickup training - example: \"username/FrozenLake-hidden-laminate\"\n",
        "adapter_subfolder = None #only specify if you want to pickup training - example: \"checkpoint-340\"\n",
        "adapter_type = \"ia3\"\n",
        "\n",
        "wandb.config.update({\n",
        "    \"base_model_name\": base_model_name,\n",
        "    \"adapter_name\": adapter_name,\n",
        "    \"adapter_subfolder\": adapter_subfolder,\n",
        "    \"adapter_type\": adapter_type,\n",
        "})\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "\n",
        "hf = HfApi()\n",
        "hf.snapshot_download(\n",
        "    repo_id=base_model_name,\n",
        "    cache_dir=\"./model/\",\n",
        "    ignore_patterns=\"*/*\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    base_model_name,\n",
        "    cache_dir=\"./model/\",\n",
        ")\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    low_cpu_mem_usage=True,\n",
        "    device_map=\"auto\",\n",
        "    cache_dir=\"./model/\",\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ")\n",
        "\n",
        "print(f\"Memory footprint: {base_model.get_memory_footprint()/1024/1024/1024} GB\")\n",
        "\n",
        "if adapter_name is not None:\n",
        "    model = PeftModel.from_pretrained(\n",
        "        base_model,\n",
        "        adapter_name,\n",
        "        adapter_name=\"<default>\",\n",
        "        subfolder=adapter_subfolder + \"/<default>\",\n",
        "        #mixed=True,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        attn_implementation=\"flash_attention_2\",\n",
        "    )\n",
        "\n",
        "    model.load_adapter(\n",
        "        adapter_name,\n",
        "        \"<delayed>\",\n",
        "        subfolder=adapter_subfolder + \"/<delayed>\",\n",
        "        is_trainable=True,\n",
        "    )\n",
        "\n",
        "else:\n",
        "    if adapter_type == \"ia3\":\n",
        "        peft_config = IA3Config(\n",
        "            #target_modules=\"all-linear\",\n",
        "            #feedforward_modules=[\"w0\"],\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            modules_to_save=[\n",
        "                #\"lm_head\",\n",
        "                #\"embed_tokens\"\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    elif adapter_type == \"lora\":\n",
        "        peft_config = LoraConfig(\n",
        "            #target_modules=\"all-linear\",\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            modules_to_save=[\n",
        "                #\"lm_head\",\n",
        "                #\"embed_tokens\"\n",
        "            ],\n",
        "            r=32,\n",
        "            lora_alpha=32,\n",
        "            lora_dropout=0.0,\n",
        "            bias=\"none\",\n",
        "            use_rslora=True,\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        raise Exception(\"Unknown adapter type\")\n",
        "\n",
        "    model = get_peft_model(\n",
        "        base_model,\n",
        "        peft_config=peft_config,\n",
        "        adapter_name=\"<default>\",\n",
        "    )\n",
        "\n",
        "    model.add_adapter(\n",
        "        peft_config=peft_config,\n",
        "        adapter_name=\"<delayed>\",\n",
        "    )\n",
        "\n",
        "model.config.use_cache = False\n",
        "\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "fR7xl1igwqih",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Uncomment desired dataset\n",
        "dataset_name = 'micahr234/FrozenLakeNotSlipperyPrepared20eto40ecombineto4096'\n",
        "#dataset_name = 'micahr234/FrozenLakeNotSlipperyPrepared20eto40ecombineto4096low'\n",
        "#dataset_name = 'micahr234/FrozenLakeNotSlipperyPrepared20eto40ecombineto4096hi'\n",
        "percentage_of_train_data = 100\n",
        "percentage_of_test_data = 100\n",
        "\n",
        "wandb.config.update({\n",
        "    \"dataset_name\": dataset_name,\n",
        "    \"percentage_of_train_data\": percentage_of_train_data,\n",
        "    \"percentage_of_test_data\": percentage_of_test_data,\n",
        "})\n",
        "\n",
        "ds_train = load_dataset(\n",
        "    dataset_name,\n",
        "    split=f\"train[:{percentage_of_train_data}%]\",\n",
        "    cache_dir=\"./dataset/\",\n",
        ")\n",
        "print(ds_train)\n",
        "\n",
        "ds_test = load_dataset(\n",
        "    dataset_name,\n",
        "    split=f\"test[:{percentage_of_test_data}%]\",\n",
        "    cache_dir=\"./dataset/\",\n",
        ")\n",
        "print(ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NyPY9J9BWJs8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import Trainer, TrainerCallback\n",
        "from collections import defaultdict\n",
        "import inspect\n",
        "\n",
        "action_value_discount = 0.9\n",
        "reward_scale = 30.0\n",
        "reward_offset = 0.0\n",
        "polyak_const = 0.1\n",
        "episode_value_discount = 0.0\n",
        "value_weight = 1.0\n",
        "imitation_weight = 0.0\n",
        "observation_weight = 0.0\n",
        "\n",
        "#os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "wandb.config.update({\n",
        "    \"action_value_discount\": action_value_discount,\n",
        "    \"reward_scale\": reward_scale,\n",
        "    \"reward_offset\": reward_offset,\n",
        "    \"polyak_const\": polyak_const,\n",
        "    \"episode_value_discount\": episode_value_discount,\n",
        "    \"value_weight\": value_weight,\n",
        "    \"imitation_weight\": imitation_weight,\n",
        "    \"observation_weight\": observation_weight,\n",
        "})\n",
        "\n",
        "def polyak_dict(source, target, tau):\n",
        "    for sk, tk in zip(source.keys(), target.keys()):\n",
        "        assert tk == sk\n",
        "        p_source = source[tk]\n",
        "        p_target = target[tk]\n",
        "        p_target.data.copy_(tau * p_source.data + (1 - tau) * p_target.data)\n",
        "\n",
        "def get_adapter_state_dict(model, adapter_name):\n",
        "    state_dict = model.state_dict()\n",
        "    adapter_state_dict = {k.replace(f\".{adapter_name}\", \".adapter_name\"): state_dict[k] for k in state_dict if f\".{adapter_name}\" in k}\n",
        "    return adapter_state_dict\n",
        "\n",
        "def index_of_next_nonzero(x, default=-1):\n",
        "    positions_of_ones = torch.nonzero(x).squeeze()\n",
        "    indices = torch.arange(x.numel(), device=x.device)\n",
        "    search_indices = torch.searchsorted(positions_of_ones, indices, right=False)\n",
        "\n",
        "    y = torch.full(x.shape, device=x.device, fill_value=default, dtype=torch.int64)\n",
        "    has_next_one = search_indices < positions_of_ones.numel()\n",
        "    y[has_next_one] = positions_of_ones[search_indices[has_next_one]]\n",
        "\n",
        "    return y\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self._stored_metrics = defaultdict(lambda: defaultdict(list))\n",
        "        self._update_count = 0\n",
        "\n",
        "    def dqn_loss(self, outputs_1, outputs_1_old, inputs, num_items_in_batch=None):\n",
        "        input_ids = inputs['input_ids'][:, 1:].detach()\n",
        "        logits_1 = outputs_1['logits'][:, :-1, ...]\n",
        "        logits_1_old = outputs_1_old['logits'][:, :-1, ...].detach()\n",
        "        action_ids_mask = inputs['action_ids_mask'][:, 1:].detach()\n",
        "        end_ids_mask = inputs['end_ids_mask'][:, 1:].detach()\n",
        "        cumulative_reward = inputs['cumulative_reward'][:, 1:].detach()\n",
        "        env_id = inputs['env_id'][:, 1:].detach()\n",
        "\n",
        "        metrics_list = []\n",
        "        loss_list = []\n",
        "        for am, em, cr, lo, olo, ii, vd in zip(action_ids_mask, end_ids_mask, cumulative_reward, logits_1, logits_1_old, input_ids, env_id):\n",
        "            with torch.no_grad():\n",
        "                nm = (~am).float()\n",
        "                am = am.float()\n",
        "                em = em.float()\n",
        "                cr = cr.float()\n",
        "\n",
        "                episode_num = torch.cumsum(em, dim=-1) - em\n",
        "\n",
        "                action_indices = torch.nonzero(am).squeeze(-1)\n",
        "                observation_indices = torch.nonzero(nm).squeeze(-1)\n",
        "\n",
        "                current_action_indices = action_indices[:-1]\n",
        "                next_action_indices = action_indices[1:]\n",
        "\n",
        "                current_action_episode_num = episode_num[current_action_indices]\n",
        "                next_action_episode_num = episode_num[next_action_indices]\n",
        "\n",
        "                current_episode_end = (current_action_episode_num != next_action_episode_num).float()\n",
        "                current_episode_reward = cr[current_action_indices]\n",
        "                current_reward = reward_scale * (current_episode_reward - reward_offset) * current_episode_end\n",
        "\n",
        "                current_action = ii[current_action_indices]\n",
        "\n",
        "            next_max_value_indices = lo[next_action_indices, :].argmax(dim=-1)\n",
        "            next_max_value = olo[next_action_indices, next_max_value_indices]\n",
        "\n",
        "            target_value = current_reward + action_value_discount * (1.0 - current_episode_end) * next_max_value\n",
        "\n",
        "            current_value = lo[current_action_indices, current_action]\n",
        "            normalized_value_loss = ((current_value - target_value) / target_value.mean().detach())**2\n",
        "\n",
        "            logprob = torch.nn.functional.log_softmax(lo, dim=-1)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                action = ii[action_indices]\n",
        "                observation = ii[observation_indices]\n",
        "\n",
        "            action_logprob = logprob[action_indices, action]\n",
        "            imitation_loss = -action_logprob\n",
        "\n",
        "            observation_logprob = logprob[observation_indices, observation]\n",
        "            observation_loss = -observation_logprob\n",
        "\n",
        "            metrics_list.append({\n",
        "                f'normalized_value_loss': normalized_value_loss,\n",
        "                f'current_value': current_value,\n",
        "                f'target_value': target_value,\n",
        "                f'imitation_loss': imitation_loss,\n",
        "                f'observation_loss': observation_loss,\n",
        "            })\n",
        "\n",
        "            loss_list.append({\n",
        "                f'normalized_value_loss': normalized_value_loss * value_weight,\n",
        "                f'imitation_loss': imitation_loss * imitation_weight,\n",
        "                f'observation_loss': observation_loss * observation_weight,\n",
        "            })\n",
        "\n",
        "        metrics = {key: torch.cat([i[key] for i in metrics_list], dim=-1).mean().detach().cpu() for key in metrics_list[0]}\n",
        "\n",
        "        loss_dict = {key: torch.cat([i[key] for i in loss_list], dim=-1).mean() for key in loss_list[0]}\n",
        "        loss = sum(loss_dict.values())\n",
        "\n",
        "        return loss, metrics\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        if self.model_accepts_loss_kwargs:\n",
        "            loss_kwargs = {}\n",
        "            if num_items_in_batch is not None:\n",
        "                loss_kwargs[\"num_items_in_batch\"] = num_items_in_batch\n",
        "            inputs = {**inputs, **loss_kwargs}\n",
        "\n",
        "        filtered_inputs = {k: v for k, v in inputs.items() if k in ['input_ids', 'attention_mask']}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.set_adapter(\"<delayed>\")\n",
        "            old_outputs = model(**filtered_inputs)\n",
        "\n",
        "        model.set_adapter(\"<default>\")\n",
        "        outputs = model(**filtered_inputs)\n",
        "\n",
        "        # Save past state if it exists\n",
        "        # TODO: this needs to be fixed and made cleaner later.\n",
        "        if self.args.past_index >= 0:\n",
        "            self._past = outputs[self.args.past_index]\n",
        "\n",
        "        loss, metrics = self.dqn_loss(outputs, old_outputs, inputs, num_items_in_batch=num_items_in_batch)\n",
        "\n",
        "        caller_function = inspect.stack()[1].function\n",
        "        if caller_function == \"training_step\":\n",
        "            train_eval = \"train\"\n",
        "            prefix = \"\"\n",
        "        elif caller_function == \"prediction_step\":\n",
        "            train_eval = \"eval\"\n",
        "            prefix = \"eval_\"\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid calling function {caller_function}\")\n",
        "\n",
        "        metrics = {f\"{prefix}{k}\": v for k, v in metrics.items()}\n",
        "        self.store_metrics(metrics, train_eval=train_eval)\n",
        "\n",
        "        if self.args.average_tokens_across_devices and self.model_accepts_loss_kwargs:\n",
        "            loss *= self.accelerator.num_processes\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def store_metrics(self, metrics, train_eval = \"train\"):\n",
        "        for key, value in metrics.items():\n",
        "            self._stored_metrics[train_eval][key].append(value)\n",
        "\n",
        "    def log(self, logs, start_time = None):\n",
        "        # logs either has 'loss' or 'eval_loss'\n",
        "        train_eval = \"train\" if \"loss\" in logs else \"eval\"\n",
        "        # Add averaged stored metrics to logs\n",
        "        for key, metrics in self._stored_metrics[train_eval].items():\n",
        "            logs[key] = torch.tensor(metrics).mean().item()\n",
        "        del self._stored_metrics[train_eval]\n",
        "\n",
        "        return super().log(logs, start_time)\n",
        "\n",
        "class polyak_update(TrainerCallback):\n",
        "    def on_optimizer_step(self, *args, **kwargs):\n",
        "        model = kwargs['model']\n",
        "        source = get_adapter_state_dict(model, \"<default>\")\n",
        "        target = get_adapter_state_dict(model, \"<delayed>\")\n",
        "        polyak_dict(source, target, polyak_const)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "Y__3gHnELy7l",
        "tags": []
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "import transformers\n",
        "\n",
        "learning_rate = 1e-2\n",
        "lr_scheduler_type = \"constant_with_warmup\"\n",
        "warmup_steps = 10\n",
        "train_batch_size = 5\n",
        "gradient_accumulation_steps = 2\n",
        "num_train_epochs = 1\n",
        "weight_decay = 0.01\n",
        "eval_steps = 10\n",
        "eval_batch_size = 5\n",
        "eval_accumulation_steps = 2\n",
        "run_eval = True\n",
        "save_steps = eval_steps\n",
        "\n",
        "#transformers.utils.logging.set_verbosity_debug()\n",
        "\n",
        "training_config = TrainingArguments(\n",
        "    output_dir=f\"./trainer/{name}/\",\n",
        "    learning_rate=learning_rate,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    warmup_steps=warmup_steps,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    eval_accumulation_steps=eval_accumulation_steps,\n",
        "    num_train_epochs=num_train_epochs,\n",
        "    fp16 = False,\n",
        "    bf16 = True,\n",
        "    fp16_full_eval = False,\n",
        "    bf16_full_eval = True,\n",
        "    optim=\"adamw_torch\",\n",
        "    weight_decay=weight_decay,\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=1,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=save_steps,\n",
        "    save_only_model=True,\n",
        "    eval_strategy=\"steps\" if run_eval else \"no\",\n",
        "    eval_steps=eval_steps,\n",
        "    disable_tqdm=True,\n",
        "    report_to=\"wandb\",\n",
        "    overwrite_output_dir=True,\n",
        "    run_name=name,\n",
        "    eval_on_start=run_eval,\n",
        "    push_to_hub=True,\n",
        "    hub_always_push=True,\n",
        "    hub_model_id=hf_name,\n",
        "    hub_private_repo=True,\n",
        "    hub_strategy=\"all_checkpoints\",\n",
        "    remove_unused_columns=False,\n",
        "    gradient_checkpointing=True,\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False,\n",
        "    },\n",
        ")\n",
        "\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_config,\n",
        "    train_dataset=ds_train,\n",
        "    eval_dataset=ds_test,\n",
        "    callbacks=[polyak_update],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s0H13ZY01vgf"
      },
      "outputs": [],
      "source": [
        "model.push_to_hub(hf_name, private=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EE_jsyLu1vgf"
      },
      "outputs": [],
      "source": [
        "wandb.run.log_code(\".\", name=name, include_fn=lambda path: path.endswith(\".ipynb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "editable": true,
        "id": "qHOmHBxt763S",
        "scrolled": true,
        "tags": []
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hgGbjEDdMlk"
      },
      "outputs": [],
      "source": [
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ciDSQhg_SrCg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}